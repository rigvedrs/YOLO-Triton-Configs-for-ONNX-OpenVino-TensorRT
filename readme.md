# Triton Inference Server Performance Optimization
## High-Performance Medical AI Model Deployment

[![Triton](https://img.shields.io/badge/NVIDIA-Triton-76B900?style=for-the-badge&logo=nvidia)](https://github.com/triton-inference-server/server)
[![TensorRT](https://img.shields.io/badge/TensorRT-Accelerated-00D2FF?style=for-the-badge&logo=nvidia)](https://developer.nvidia.com/tensorrt)
[![ONNX](https://img.shields.io/badge/ONNX-Runtime-005CED?style=for-the-badge&logo=onnx)](https://onnxruntime.ai/)
[![OpenVINO](https://img.shields.io/badge/Intel-OpenVINO-0071C5?style=for-the-badge&logo=intel)](https://docs.openvino.ai/)

This repository contains a comprehensive performance optimization study for deploying a **chest X-ray detection model** using NVIDIA Triton Inference Server. We benchmark multiple inference backends and configurations to achieve optimal throughput and latency for medical AI applications.

---

## üöÄ Performance Results Overview

Our optimization journey achieved remarkable performance improvements through systematic backend comparison and configuration tuning:

### üèÜ Top Performers

| Configuration | Throughput (infer/sec) | Avg Latency (ms) | Performance Gain |
|---------------|------------------------|------------------|------------------|
| **Custom TensorRT V2** | **157.8** | **40.6** | **Best Overall** |
| Default TensorRT | 129.0 | 54.6 | 62% faster than ONNX |
| Custom TensorRT V1 | 113.6 | 19.3 | Lowest latency |
| Default ONNX GPU | 79.6 | 91.7 | Baseline |
| Custom ONNX GPU | 78.3 | 89.6 | Similar to baseline |

### üí° Key Optimization Breakthrough

The **Custom TensorRT V2** configuration achieved a **98% throughput increase** over baseline ONNX by simply increasing the `max_workspace_size_bytes` from 3GB to 8GB, demonstrating the critical impact of TensorRT memory allocation optimization.

---

## üèóÔ∏è Architecture & Configurations

### Model Specifications
- **Input Shape**: `[3, 640, 640]` (RGB image)
- **Output Shape**: `[84, -1]` (YOLO-style detection)
- **Batch Size**: 1 (optimized for real-time inference)
- **Concurrency**: 8 concurrent requests

### Backend Configurations

#### üî• TensorRT Configurations

<details>
<summary><strong>Custom TensorRT V2 (Best Performance)</strong></summary>

```protobuf
optimization {
  execution_accelerators {
    gpu_execution_accelerator {
      name: "tensorrt"
      parameters {
        key: "precision_mode"
        value: "FP16"
      }
      parameters {
        key: "max_workspace_size_bytes"
        value: "8589934592"  # 8GB - Key optimization!
      }
      parameters {
        key: "trt_engine_cache_enable"
        value: "1"
      }
    }
  }
}
```
**Performance**: 157.8 infer/sec, 40.6ms latency
</details>

<details>
<summary><strong>Custom TensorRT V1 (Lowest Latency)</strong></summary>

```protobuf
max_batch_size: 16
dynamic_batching {
  preferred_batch_size: [4, 8, 16]
  max_queue_delay_microseconds: 100
}
instance_group [{
  count: 2
  kind: KIND_GPU
  gpus: [0]
}]
```
**Performance**: 113.6 infer/sec, 19.3ms latency
</details>

#### ‚ö° ONNX Runtime Configurations

<details>
<summary><strong>Custom ONNX GPU</strong></summary>

```protobuf
platform: "onnxruntime_onnx"
max_batch_size: 32
dynamic_batching {
  preferred_batch_size: [4, 8, 16]
  max_queue_delay_microseconds: 100
}
optimization {
  execution_accelerators {
    gpu_execution_accelerator: [{
      name: "cuda"
    }]
  }
}
```
**Performance**: 78.3 infer/sec, 89.6ms latency
</details>

#### üñ•Ô∏è CPU-Optimized Configurations

<details>
<summary><strong>OpenVINO (Intel Optimization)</strong></summary>

```protobuf
backend: "openvino"
instance_group [{
  count: 1
  kind: KIND_CPU
}]
```
**Performance**: 14.6 infer/sec, 536ms latency (Nano configuration)
</details>

---

## üìä Detailed Performance Analysis

### Throughput Comparison
```
Custom TensorRT V2  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 157.8 infer/sec
Default TensorRT    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       129.0 infer/sec  
Custom TensorRT V1  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          113.6 infer/sec
Default ONNX        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  79.6 infer/sec
Custom ONNX         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  78.3 infer/sec
OpenVINO Nano       ‚ñà‚ñà‚ñà                               14.6 infer/sec
ONNX CPU            ‚ñå                                  1.0 infer/sec
OpenVINO V1         ‚ñå                                  1.0 infer/sec
```

### Latency Distribution Analysis

| Configuration | P50 (ms) | P90 (ms) | P95 (ms) | P99 (ms) | Std Dev (ms) |
|---------------|----------|----------|----------|----------|--------------|
| Custom TensorRT V1 | 18.5 | 24.2 | 27.0 | 31.0 | 3.9 |
| Custom TensorRT V2 | 41.9 | 55.7 | 59.2 | 65.3 | 12.4 |
| Default TensorRT | 55.0 | 69.2 | 72.5 | 79.5 | 11.3 |
| Custom ONNX | 88.1 | 105.0 | 123.7 | 139.5 | 14.3 |
| Default ONNX | 91.6 | 95.7 | 96.5 | 114.1 | 7.2 |

---

## üîß Quick Start

### Prerequisites
- NVIDIA GPU with CUDA support
- Docker with NVIDIA Container Runtime
- NVIDIA Triton Inference Server
- Chest X-ray detection model (ONNX format)

### Launch Triton Server
```bash
# Start Triton with model repository
docker run --gpus all --rm \
  -p 8000:8000 -p 8001:8001 -p 8002:8002 \
  -v $(pwd)/model_repository:/models \
  nvcr.io/nvidia/tritonserver:23.10-py3 \
  tritonserver --model-repository=/models
```

### Run Performance Benchmarks
```bash
# TensorRT optimization benchmark
perf_analyzer -u triton_server:8000 \
  -m chest_xray_detector \
  --input-data input_alt1.json \
  -b 1 --shape images:3,640,640 \
  --concurrency-range 8

# ONNX baseline benchmark  
perf_analyzer -u triton_server:8000 \
  -m chest_xray_detector \
  --input-data input_fixed.json \
  -b 1 --shape images:3,640,640 \
  --concurrency-range 8
```

---

## üìÅ Repository Structure

```
‚îú‚îÄ‚îÄ compiled.txt                    # Performance benchmark results
‚îú‚îÄ‚îÄ custom1_tensorRT.pbtxt         # TensorRT config with batching
‚îú‚îÄ‚îÄ custom2_tensorRT.pbtxt         # TensorRT config V2 (best performance)  
‚îú‚îÄ‚îÄ custom_onnx.pbtxt              # ONNX GPU configuration
‚îú‚îÄ‚îÄ nano_custom_onnx.pbtxt         # OpenVINO Nano configuration
‚îú‚îÄ‚îÄ openvino.pbtxt                 # OpenVINO standard configuration
‚îú‚îÄ‚îÄ yolo_default_onnx.pbtxt        # Default ONNX baseline
‚îî‚îÄ‚îÄ yolo_default_tensorRT.pbtxt    # Default TensorRT baseline
```

---

## üéØ Key Optimization Insights

### üöÄ Memory Management is Critical
- Increasing TensorRT workspace from 3GB to 8GB yielded **22% throughput improvement**
- Memory allocation directly impacts kernel fusion and optimization opportunities

### ‚öñÔ∏è Latency vs Throughput Trade-offs
- **Custom TensorRT V1**: Ultra-low latency (19.3ms) with dynamic batching
- **Custom TensorRT V2**: Maximum throughput (157.8 infer/sec) with higher memory allocation

### üñ•Ô∏è CPU Backends for Edge Deployment
- **OpenVINO Nano**: 15x faster than standard OpenVINO on CPU-only systems
- Suitable for edge deployment where GPU resources are unavailable

### üìà Dynamic Batching Benefits
- Configurations with dynamic batching show improved resource utilization
- Optimal batch sizes: `[4, 8, 16]` for this model architecture

---

## üî¨ Technical Deep Dive

### TensorRT Optimization Parameters

| Parameter | Default | Optimized | Impact |
|-----------|---------|-----------|---------|
| `precision_mode` | FP32 | FP16 | 2x memory efficiency |
| `max_workspace_size_bytes` | 3GB | 8GB | +22% throughput |
| `trt_engine_cache_enable` | 0 | 1 | Faster cold starts |
| Dynamic batching | Disabled | Enabled | Better utilization |

### ONNX Runtime Acceleration
- CUDA execution provider enables GPU acceleration
- Dynamic input shapes support variable image sizes
- Memory pool optimization reduces allocation overhead

---

## üéØ Use Cases & Applications

### üè• Medical AI Deployment
- **Real-time chest X-ray screening**
- **Batch processing for radiological studies**  
- **Edge deployment in resource-constrained environments**

### üîÑ Model Serving Scenarios
- **High-throughput**: Custom TensorRT V2 for maximum requests/second
- **Low-latency**: Custom TensorRT V1 for real-time applications
- **CPU-only**: OpenVINO Nano for edge devices

---

## üõ†Ô∏è Configuration Recommendations

### For Maximum Throughput
```protobuf
# Use Custom TensorRT V2 configuration
max_workspace_size_bytes: "8589934592"  # 8GB
precision_mode: "FP16"
```

### For Minimum Latency  
```protobuf
# Use Custom TensorRT V1 with multiple instances
instance_group [{ count: 2, kind: KIND_GPU }]
dynamic_batching { max_queue_delay_microseconds: 100 }
```

### For CPU Deployment
```protobuf
# Use OpenVINO Nano configuration
backend: "openvino"
kind: KIND_CPU
```

---

## üìà Performance Monitoring

Monitor these key metrics during deployment:

- **Throughput**: Target >100 infer/sec for production
- **P95 Latency**: Keep <100ms for real-time applications  
- **GPU Utilization**: Optimize for >80% utilization
- **Memory Usage**: Monitor workspace allocation efficiency

---

## ü§ù Contributing

We welcome contributions to improve performance benchmarks and add new backend configurations:

1. Fork the repository
2. Add new configuration files
3. Run performance benchmarks  
4. Submit pull request with results

---

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## üôè Acknowledgments

- **NVIDIA Triton Inference Server** team for the robust serving platform
- **TensorRT** team for optimization framework
- **ONNX Runtime** community for cross-platform inference
- **Intel OpenVINO** team for CPU optimization tools

---

<div align="center">

**Built with ‚ù§Ô∏è for high-performance medical AI deployment**

[Report Bug](../../issues) ¬∑ [Request Feature](../../issues) ¬∑ [Documentation](../../wiki)

</div>